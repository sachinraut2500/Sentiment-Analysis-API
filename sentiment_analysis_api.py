# -*- coding: utf-8 -*-
"""Sentiment Analysis API

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a-DaSwyTIVpHrugM4-3iD72lN-5X1nEi
"""

# sentiment_analysis_api.py
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import re
import pickle
import json
from flask import Flask, request, jsonify
from flask_cors import CORS
import logging

# Download NLTK data
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('punkt')
    nltk.download('stopwords')
    nltk.download('wordnet')

class SentimentAnalyzer:
    def __init__(self, vocab_size=10000, max_length=100, embedding_dim=100):
        self.vocab_size = vocab_size
        self.max_length = max_length
        self.embedding_dim = embedding_dim
        self.tokenizer = None
        self.model = None
        self.lemmatizer = WordNetLemmatizer()
        self.stop_words = set(stopwords.words('english'))

    def preprocess_text(self, text):
        """Clean and preprocess text data"""
        # Convert to lowercase
        text = text.lower()

        # Remove URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

        # Remove user mentions and hashtags
        text = re.sub(r'@\w+|#\w+', '', text)

        # Remove punctuation and numbers
        text = re.sub(r'[^a-zA-Z\s]', '', text)

        # Tokenize
        tokens = word_tokenize(text)

        # Remove stopwords and lemmatize
        tokens = [self.lemmatizer.lemmatize(token) for token in tokens
                 if token not in self.stop_words and len(token) > 2]

        return ' '.join(tokens)

    def prepare_data(self, texts, labels=None):
        """Prepare data for training or prediction"""
        # Preprocess texts
        processed_texts = [self.preprocess_text(text) for text in texts]

        if self.tokenizer is None:
            # Create and fit tokenizer
            self.tokenizer = Tokenizer(num_words=self.vocab_size, oov_token='<OOV>')
            self.tokenizer.fit_on_texts(processed_texts)

        # Convert texts to sequences
        sequences = self.tokenizer.texts_to_sequences(processed_texts)

        # Pad sequences
        padded_sequences = pad_sequences(sequences, maxlen=self.max_length, padding='post')

        if labels is not None:
            return padded_sequences, np.array(labels)
        return padded_sequences

    def build_model(self, model_type='lstm'):
        """Build neural network model"""
        self.model = Sequential()

        # Embedding layer
        self.model.add(Embedding(
            input_dim=self.vocab_size,
            output_dim=self.embedding_dim,
            input_length=self.max_length
        ))

        if model_type.lower() == 'lstm':
            # LSTM layers
            self.model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))
            self.model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))

        elif model_type.lower() == 'bilstm':
            # Bidirectional LSTM layers
            self.model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))
            self.model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))

        # Dense layers
        self.model.add(Dense(64, activation='relu'))
        self.model.add(Dropout(0.5))
        self.model.add(Dense(32, activation='relu'))
        self.model.add(Dropout(0.3))

        # Output layer (3 classes: negative, neutral, positive)
        self.model.add(Dense(3, activation='softmax'))

        # Compile model
        self.model.compile(
            optimizer='adam',
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )

        print(f"{model_type.upper()} model built successfully")
        return self.model

    def train(self, X_train, y_train, X_val, y_val, epochs=50, batch_size=32):
        """Train the model"""
        # Convert labels to categorical
        y_train_cat = tf.keras.utils.to_categorical(y_train, 3)
        y_val_cat = tf.keras.utils.to_categorical(y_val, 3)

        # Callbacks
        callbacks = [
            tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),
            tf.keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=3),
            tf.keras.callbacks.ModelCheckpoint('best_sentiment_model.h5', save_best_only=True)
        ]

        # Train model
        history = self.model.fit(
            X_train, y_train_cat,
            validation_data=(X_val, y_val_cat),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )

        return history

    def predict_sentiment(self, text):
        """Predict sentiment of a single text"""
        if isinstance(text, str):
            text = [text]

        # Preprocess and prepare data
        processed_data = self.prepare_data(text)

        # Make prediction
        predictions = self.model.predict(processed_data)

        # Convert to sentiment labels
        sentiment_labels = ['Negative', 'Neutral', 'Positive']
        results = []

        for pred in predictions:
            sentiment_idx = np.argmax(pred)
            confidence = pred[sentiment_idx]

            results.append({
                'sentiment': sentiment_labels[sentiment_idx],
                'confidence': float(confidence),
                'scores': {
                    'negative': float(pred[0]),
                    'neutral': float(pred[1]),
                    'positive': float(pred[2])
                }
            })

        return results[0] if len(results) == 1 else results

    def evaluate(self, X_test, y_test):
        """Evaluate model performance"""
        # Convert to categorical
        y_test_cat = tf.keras.utils.to_categorical(y_test, 3)

        # Make predictions
        predictions = self.model.predict(X_test)
        predicted_classes = np.argmax(predictions, axis=1)

        # Classification report
        sentiment_labels = ['Negative', 'Neutral', 'Positive']
        report = classification_report(y_test, predicted_classes, target_names=sentiment_labels)
        print("Classification Report:")
        print(report)

        # Confusion matrix
        cm = confusion_matrix(y_test, predicted_classes)
        print("Confusion Matrix:")
        print(cm)

        return report, cm

    def save_model(self, model_path='sentiment_model.h5', tokenizer_path='tokenizer.pickle'):
        """Save model and tokenizer"""
        self.model.save(model_path)

        with open(tokenizer_path, 'wb') as f:
            pickle.dump(self.tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)

        print(f"Model saved to {model_path}")
        print(f"Tokenizer saved to {tokenizer_path}")

    def load_model(self, model_path='sentiment_model.h5', tokenizer_path='tokenizer.pickle'):
        """Load model and tokenizer"""
        self.model = tf.keras.models.load_model(model_path)

        with open(tokenizer_path, 'rb') as f:
            self.tokenizer = pickle.load(f)

        print("Model and tokenizer loaded successfully")

# Flask API
app = Flask(__name__)
CORS(app)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize sentiment analyzer
sentiment_analyzer = None

def initialize_model():
    """Initialize the sentiment analysis model"""
    global sentiment_analyzer
    try:
        sentiment_analyzer = SentimentAnalyzer()
        sentiment_analyzer.load_model()
        logger.info("Model loaded successfully")
    except Exception as e:
        logger.error(f"Error loading model: {e}")
        # Train a new model if loading fails
        train_new_model()

def train_new_model():
    """Train a new sentiment analysis model"""
    global sentiment_analyzer

    # Sample training data (replace with real dataset)
    sample_data = {
        'text': [
            "I love this product, it's amazing!",
            "This is terrible, worst purchase ever",
            "It's okay, nothing special",
            "Absolutely fantastic, highly recommend",
            "Not good at all, disappointed",
            "Average product, decent quality",
        ],
        'label': [2, 0, 1, 2, 0, 1]  # 0: negative, 1: neutral, 2: positive
    }

    df = pd.DataFrame(sample_data)

    # Prepare data
    X, y = sentiment_analyzer.prepare_data(df['text'].tolist(), df['label'].tolist())

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

    # Build and train model
    sentiment_analyzer.build_model('bilstm')
    sentiment_analyzer.train(X_train, y_train, X_val, y_val, epochs=10)

    # Save model
    sentiment_analyzer.save_model()

    logger.info("New model trained and saved")

@app.route('/api/predict', methods=['POST'])
def predict():
    """API endpoint for sentiment prediction"""
    try:
        data = request.get_json()

        if 'text' not in data:
            return jsonify({'error': 'Text field is required'}), 400

        text = data['text']
        if not text or not isinstance(text, str):
            return jsonify({'error': 'Valid text string is required'}), 400

        # Make prediction
        result = sentiment_analyzer.predict_sentiment(text)

        return jsonify({
            'success': True,
            'text': text,
            'prediction': result
        })

    except Exception as e:
        logger.error(f"Prediction error: {e}")
        return jsonify({'error': 'Internal server error'}), 500

@app.route('/api/batch_predict', methods=['POST'])
def batch_predict():
    """API endpoint for batch sentiment prediction"""
    try:
        data = request.get_json()

        if 'texts' not in data or not isinstance(data['texts'], list):
            return jsonify({'error': 'Texts array is required'}), 400

        texts = data['texts']
        if len(texts) > 100:  # Limit batch size
            return jsonify({'error': 'Maximum 100 texts allowed per batch'}), 400

        # Make predictions
        results = []
        for text in texts:
            if isinstance(text, str) and text.strip():
                result = sentiment_analyzer.predict_sentiment(text)
                results.append({
                    'text': text,
                    'prediction': result
                })
            else:
                results.append({
                    'text': text,
                    'error': 'Invalid text'
                })

        return jsonify({
            'success': True,
            'results': results
        })

    except Exception as e:
        logger.error(f"Batch prediction error: {e}")
        return jsonify({'error': 'Internal server error'}), 500

@app.route('/api/health', methods=['GET'])
def health():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'model_loaded': sentiment_analyzer is not None and sentiment_analyzer.model is not None
    })

@app.route('/api/info', methods=['GET'])
def info():
    """API information endpoint"""
    return jsonify({
        'name': 'Sentiment Analysis API',
        'version': '1.0.0',
        'description': 'Deep learning-based sentiment analysis for text data',
        'endpoints': {
            'predict': 'POST /api/predict',
            'batch_predict': 'POST /api/batch_predict',
            'health': 'GET /api/health',
            'info': 'GET /api/info'
        },
        'sentiment_classes': ['Negative', 'Neutral', 'Positive']
    })

def main():
    """Main function for training and testing"""
    # Initialize analyzer
    analyzer = SentimentAnalyzer()

    # Load sample data (replace with real dataset)
    # For example, you can use the IMDB movie reviews dataset
    # or Twitter sentiment dataset

    # Sample data for demonstration
    sample_texts = [
        "I absolutely love this movie! It's fantastic.",
        "This product is terrible, waste of money.",
        "It's an okay product, nothing special.",
        "Amazing quality, highly recommended!",
        "Poor customer service, very disappointed.",
        "Average experience, could be better."
    ]

    sample_labels = [2, 0, 1, 2, 0, 1]  # 0: negative, 1: neutral, 2: positive

    # Prepare data
    X, y = analyzer.prepare_data(sample_texts, sample_labels)

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Build and train model
    analyzer.build_model('bilstm')
    history = analyzer.train(X_train, y_train, X_test, y_test, epochs=10)

    # Evaluate model
    analyzer.evaluate(X_test, y_test)

    # Test predictions
    test_texts = [
        "This is amazing!",
        "I hate this product",
        "It's okay, nothing special"
    ]

    for text in test_texts:
        result = analyzer.predict_sentiment(text)
        print(f"Text: {text}")
        print(f"Sentiment: {result['sentiment']} (Confidence: {result['confidence']:.3f})")
        print("-" * 50)

    # Save model
    analyzer.save_model()

if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == 'train':
        main()
    else:
        # Initialize model and start API server
        initialize_model()
        app.run(host='0.0.0.0', port=5000, debug=True)